# 1.5 决策树 (Decision Tree)
## 1.5.1 决策树介绍 (Introduction to Decision Tree)

决策树(Decision Tree)是解决分类任务的另一种思路，它是其它所有树结构模型的基础模型。决策树解决分类任务的过程非常直接明了，具有可解释性，且符合日常生活中人们做决策的思维过程。例如，一个广为流传的关于一个人是否适合读博士的自我判断标准，就是一棵决策树如图所示1.5.1。

<center>
    <img  src="ML\ML_figure\decision_tree.png" width="40%">
    <br>
    <div style="margin-bottom: 50px; margin-top: 20px">
        <font size="3">图1.5.1 Example of Decision Tree</font>
    </div>
</center>

从图1.5.1中可以看出，决策树的结构包括叶节点和内部节点。其中叶节点代表决策树输出分类类别，如图中的决策树的输出为一个人“是”或“否”适合读博士；内部节点为数据某一特征的划分决策节点，如一个人“是否热爱做研究”、“是否想成为教授”等划分节点。决策树对样本类别的判断流程是一个分而治之的过程，我们要做的是构造一棵能选择合适特征属性的决策树来合理地拆分样本。

显然，构建决策树最关键的一步就是如何合理安排特征属性的先后顺序作为决策判断点。决策树算法发展至今，主流的决策树算法有三种：ID3决策树、C4.5决策树、CART决策树。这三种不同的决策树算法区别在于它们选取划分节点的方法不同。其中ID3决策树是通过计算信息增益(Information Gain)来选择；C4.5决策树是通过信息增益率(Information Gain Ratio)来选择；CART决策树是通过计算基尼指数(Gini Index)来选择。

## 1.5.2 信息量与信息熵 (Information and Information Entropy)

信息熵(Information Entropy)是量化数据样本信息量的一种指标，它度量了样本信息中的不确定性，当一件事的发生的概率小，信息量就越大。在计算信息熵之前，我们要考虑如何将一个事件反映的信息进行量化，即信息量。如图1.5.2所示，有8支队伍参加比赛，并且假设每只队伍相互之间取得胜利的概率为$\frac{1}{2}$。假如队伍2从八强获得冠军，概率为$\frac{1}{8}$；若队伍2从四强获得冠军，则概率为$\frac{1}{4}$。显然，队伍2从四强获得冠军的信息量要比从八强获得冠军的信息量要大，因为从八强到冠军比从四强到冠军的不确定性高。从图中得知，获取队伍2从八强到冠军的信息量有两条路径：其一，先进入决赛的信息量，再赢得决赛取得冠军的信息量；其二，直接获取队伍2从八强到冠军的信息量。无论那一条路径，我们最终得到的都是队伍2从八强获得冠军的信息量。那么就存在一个计算信息量的等式令两条路径的信息量相等：$f(\frac{1}{8})=f(\frac{1}{4})+f(\frac{1}{2})$。

<center>
    <img  src="ML\ML_figure\champion.png" width="50%">
    <br>
    <div style="margin-bottom: 50px; margin-top: 20px">
        <font size="3">图1.5.2 Route of winning the game</font>
    </div>
</center>

那么如何设计$f(\cdot)$才能让等式成立呢？从概率的角度去思考，队伍2夺冠的概率为$p = \frac{1}{8}$，就相当于令队伍2进入决赛的概率（$p_1 = \frac{1}{4}$）与赢得决赛的概率（$p_2 = \frac{1}{2}$）相乘，即$p=p1\cdot p2$。为了让上述信息量等式成立，我们对概率计算的公式取对数，就能让$f(\cdot)$等式成立，即$-\log_2 p = -(\log_2 p_1 + \log_2 p_2) $。因此，基于事件概率$p$，信息量可以定义为：

$$
f(p)=-\log_2 p
\tag{1.5.1}
$$
其中，由于log是单调递增的，而我们希望发生事件的概率与信息量成反比，因此需要在对数前加上负号。对数的底数为2的原因是，我们在计算信息量的时候，希望将一件事情的信息量能够直观的表达，以2为底就意味着计算出来的单位是计算机中的存储单位比特(Bit)。例如，一件$p = \frac{1}{8}$可能性发生的事情发生了，信息的不确定性就从$p = \frac{1}{8}$变成了$p = \frac{1}{1}$，相当于在二进制存储中减少了8位数字的信息量。

若图1.5.1中的队伍1和队伍2相互之间取得胜利的概率不再是$\frac{1}{2}$，而是$\frac{1}{100}$与$\frac{99}{100}$，那么某一支队伍获得胜利的信息量就不能直接相加了。因为获胜概率为$\frac{99}{100}$的队伍1取得胜利的不确定性很低，而获胜概率为$\frac{1}{100}$的队伍2取得胜利的不确定性却很高，即信息量多。但是，只有当队伍2取得胜利的事件发生了，整个事件才能贡献出$-\log_2 \frac{1}{100} $的信息量。也就是说我们需要对信息量乘以它的概率作为权重，来避免这种不平衡的情况。因此，基于事件发生的概率$p_i$，信息熵就可以定义为对信息量的期望，即：

$$
\operatorname{H}(P)=-\sum_{i=1}^{m} p_{i} \log _{2} p_{i}
\tag{1.5.2}
$$

## 1.5.3 信息增益与ID3决策树 (Information Gain and ID3 Decision Tree)

ID3决策树通过计算信息增益(Information Gain)选择划分节点，计算信息增益先要计算信息熵。假设数据集样本$D$中有$C$个类别，且第$c$类数据样本所占的比例为$p_c$，那么数据集$D$的信息熵$\operatorname{H}(D)$为：

$$
\operatorname{H}(D)=-\sum_{c=1}^{C} p_{c} \log _{2} p_{c}
\tag{1.5.3}
$$

其中，信息熵$\operatorname{H}(D)$为正数。样本的信息不确定性越高，信息熵越大；反之，若信息纯度越高，信息熵越小。信息不确定性的意思是没办法通过一个特征属性来完全区分出样本的类别。而在构建决策树的过程中，我们所希望的就是决策树中每个划分节点能尽可能将样本分出来的类别相同（纯度高）。因此，我们需要计算数据集样本$D$在每个特征属性下的信息熵来计算出一个特征属性的信息增益是多少。

假设数据集中有一个离散特征属性为$a$，且有$V$个取值$\{a_1, a_2, \dots, a_V\}$。那么在使用$a$作为划分节点的特征属性时，就会有$V$个新分支。在第$v$个新分支中分划出了数据集$D$里全部是$a$特征且值为$a_v$的数据样本子集$D_v$。那么一个特征属性$a$条件下的信息熵$\operatorname{H}(D, a)$可以通过$V$个不同取值特征$a_v$的数据样本子集$D_v$进行计算：

$$
\operatorname{H}(D , a)=\sum_{v=1}^{V} \frac{\left|D_{v}\right|}{|D|} \operatorname{H}\left(D_{v}\right)
\tag{1.5.4}
$$

其中，$|D|$为完整数据集的样本数，$\left|D_{v}\right|$为特征$a$取值为$a_v$的数据子集$Dv$的样本数。$ \frac{\left|D_{v}\right|}{|D|}$为特征取值样本权重比，其作用是为了让样本数越多的特征取值分支影响越大。最终，特征属性$a$对数据集$D$进行划分的信息增益为：

$$
\operatorname{I}(D, a)=\operatorname{H}(D)-\operatorname{H}(D , a)
\tag{1.5.5}
$$

之前提到，我们所希望的就是决策树中每个划分节点能尽可能将样本分出来的类别相同（纯度高）。因此，在数据集信息熵$\operatorname{H}(D)$不变的情况下，某一特征$a$的信息熵$\operatorname{H}(D , a)$越小划分出来的数据样本纯度越高，这意味着我们希望用信息增益$\operatorname{I}(D, a)$越大的特征属性来作为划分节点。

最后，构建一棵ID3决策树的过程为从上至下分别选择不同特征属性作为划分节点。选择第一个划分节点所使用的是完整数据集$D$来计算出各个特征属性$a$的信息增益$\operatorname{I}(D, a)$，然后选取信息增益最大的特征来作为该特征划分节点。接下来的特征根据划分第一个节点时所分出的$V$个新分支的数据子集$D_v$来作为新的划分数据集，重复之前的操作计算出不同属性的信息增益$\operatorname{I}(D_v, a)$，直到数据集$D$中的样本全部被分类完毕。

然而，如果数据集中出现如“索引值”这样的特征，假如数据集中有$n$个样本对应$n$个索引值，那么就会出现以“索引值”为划分节点的$n$个新分支，且每个分支只有一个样本。而因为“索引值”能个正确分类这$n$个分支中的每个样本，会被决策树认为“索引值”的纯度很高，对这一类取值数目多的特征属性有所偏好。这样以来，ID3决策树不仅没有办法很好的完成分类任务，而且会导致模型泛化能力低、出现过拟合现象。

## 1.5.4 信息增益率与C4.5决策树 (Information Gain Ratio and C4.5 Decision Tree)

为了解决ID3决策树的不足，C4.5决策树使用计算信息增益率(Information Gain Ratio)的策略来选择划分节点。计算特征属性$a$的信息增益率要先计算特征属性$a$的特征熵，也称固有值(Intrinsic Value)。特征属性$a$的特征熵定义为：

$$
\operatorname{H}_{a}(D)=-\sum_{v=1}^{V} \frac{\left|D_{v}\right|}{|D|} \log _{2} \frac{\left|D_{v}\right|}{|D|}
\tag{1.5.6}
$$
特征熵是关于属性$a$取值为$a_v$的样本子集$D_v$的信息熵，因此特征熵也为正数。特征熵反应的是特征取值数的多少，特征取值数越多，特征熵越大。基于特征熵$\operatorname{H}_{a}(D)$，信息增益率定义为：

$$
\operatorname{I}_{\operatorname{R}}(D, A)=\frac{\operatorname{I}(D, a)}{\operatorname{H}_{a}(D)}
\tag{1.5.7}
$$
由于特征取值数越多，特征熵越大，因此特征熵作为计算信息增益率的分母，它能调节改善信息增益对某一取值数目多的特征属性有所偏好的现象。

C4.5决策树在构造决策树的过程中不是像ID3一样直接选取最大信息增益一样，选取最大信息增益率的特征作为划分节点。C4.5在划分节点的过程中，先会从候选特征属性中选取那些高于平均信息增益的特征，然后再从中选择信息增益率最高的特征作为划分节点。


## 1.5.5 基尼指数与CART决策树 (Gini Index and CART Decision Tree)

CART决策树与ID3、C4.5决策树很相似，都是通过计算出某一特征的加权信息纯度来选择特征作为对划分节点。然而，不同点是CART决策树采用的是计算基尼系数(Gini Index)来计算数据样本的纯度，代替了ID3与C4.5计算熵的方法，从而减少计算机的算力消耗。同样地，假设数据集样本$D$中有$C$个类别，且第$c$类数据样本所占的比例为$p_c$，那么整个数据集$D$的基尼指数定义为：

$$
\begin{aligned}
\operatorname{Gini}(D)&=\sum_{c=1}^{C} p_{c}\left(1-p_{c}\right) \\
&=\sum_{c=1}^{C} p_k  + \sum_{c=1}^{C} p_{k}^{2} \\
&=1-\sum_{c=1}^{C} p_{k}^{2}
\end{aligned}
\tag{1.5.8}
$$
其中$1-{p_c}$为出现与类别$c$不是同一类别（其他类别）的概率，那么${p_c}(1-{p_c})$就表示从数据集$D$中随机抽取$C$个不同类别的样本，它们不一致的概率是多少。因此当基尼系数越小，类别不一致的概率就越小，则证明类别纯度就越高。例如，图1.5.3为二分类中基尼系数的变化，当其中某一类别出现的概率达到最高时（为$1$），另外一类别的概率就会为$0$，这种情况下基尼系数达到最小，即类别纯度最高；反之，当两个类别可能同时出现的概率为50\%的时候，基尼系数达到最大，即类别纯度最低。若有特征属性$a$取值为$a_v$及其样本子集为$D_v$，那么特征$a$的基尼系数$\operatorname{Gini}(D,a)$则定义为：

$$
\operatorname{Gini}(D, a)=\sum_{v=1}^{V} \frac{\left|D_{v}\right|}{|D|} \operatorname{Gini}\left(D_{v}\right)
\tag{1.5.9}
$$

<center>
    <img  src="ML\ML_figure\gini_index.png" width="40%">
    <br>
    <div style="margin-bottom: 50px; margin-top: 20px">
        <font size="3">图1.5.3 Gini Index</font>
    </div>
</center>

最后，与ID3决策树相反，在构建决策树时，选取最小基尼指数的特征作为划分节点。由于CART决策树计算的是相同类别样本出现的概率，而不是信息熵，因此不需要考虑决策树会对某些多取值特征的偏好问题。
